# üß™ Visualizaci√≥n de la frontera de decisi√≥n usando MLP con el dataset Iris (2 caracter√≠sticas)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neural_network import MLPClassifier

# 1Ô∏è‚É£ Cargar el dataset Iris y usar solo dos caracter√≠sticas para poder graficar en 2D
iris = load_iris()
X = iris.data[:, :2]  # Solo las primeras 2 caracter√≠sticas: s√©palo largo y ancho
y = iris.target
nombres_clases = iris.target_names

# 2Ô∏è‚É£ Normalizar los datos para que todas las caracter√≠sticas est√©n en la misma escala
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# 3Ô∏è‚É£ Dividir los datos en entrenamiento y prueba (80% - 20%)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 4Ô∏è‚É£ Definir una grilla amplia de hiperpar√°metros para buscar la mejor configuraci√≥n del MLP
param_grid = {
    "hidden_layer_sizes": [
        (2,),
        (4,),
        (4, 2),
        (8,),
        (8, 4),
        (8, 4, 2),
        (16,),
        (16, 8),
        (16, 8, 4),
        (16, 8, 4, 2),
        (32,),
        (32, 16),
        (32, 16, 8),
        (32, 16, 8, 4),
        (32, 16, 8, 4, 2),
        (64,),
        (64, 32),
        (64, 32, 16),
        (64, 32, 16, 8),
        (64, 32, 16, 8, 4),
        (64, 32, 16, 8, 4, 2),
    ],
    "alpha": [1e-5, 1e-4, 1e-3, 1e-2],  # Par√°metro de regularizaci√≥n L2
    "activation": ["relu", "tanh"],  # Funciones de activaci√≥n comunes
}

# 5Ô∏è‚É£ Instanciar el modelo base de MLP (optimizador Adam, sin definir arquitectura a√∫n)
mlp = MLPClassifier(solver="adam", max_iter=3000, random_state=42)

# 6Ô∏è‚É£ Realizar b√∫squeda en grilla con validaci√≥n cruzada de 3 folds
grid = GridSearchCV(
    estimator=mlp, param_grid=param_grid, scoring="accuracy", cv=3, n_jobs=-1, verbose=2
)
grid.fit(X_train, y_train)

# 7Ô∏è‚É£ Mostrar el mejor modelo encontrado
print(f"\nüèÜ Mejor accuracy validaci√≥n cruzada: {grid.best_score_:.4f}")
print("üîß Mejores hiperpar√°metros encontrados:")
for param, value in grid.best_params_.items():
    print(f"   {param}: {value}")

# 8Ô∏è‚É£ Evaluar el modelo √≥ptimo sobre el conjunto de prueba
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

# 9Ô∏è‚É£ Generar una malla (rejilla) de puntos sobre todo el espacio 2D para visualizar la frontera
h = 0.001  # paso muy fino para mayor precisi√≥n visual
x_min, x_max = X_scaled[:, 0].min() - 0.1, X_scaled[:, 0].max() + 0.1
y_min, y_max = X_scaled[:, 1].min() - 0.1, X_scaled[:, 1].max() + 0.1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
puntos_malla = np.c_[
    xx.ravel(), yy.ravel()
]  # Convertir malla a array de coordenadas 2D

# üîü Predecir clase para cada punto de la malla
Z = best_model.predict(puntos_malla)
Z = Z.reshape(xx.shape)  # Volver a forma de matriz para graficar

# üîÅ Visualizaci√≥n de la frontera de decisi√≥n y datos reales
plt.figure(figsize=(8, 6))
plt.contourf(
    xx,
    yy,
    Z,
    alpha=0.3,
    levels=[-0.5, 0.5, 1.5, 2.5],
    colors=["#FFAAAA", "#AAFFAA", "#AAAAFF"],
)

# üîπ Puntos de entrenamiento y prueba con etiquetas de clase
plt.scatter(
    X_train[:, 0], X_train[:, 1], c=y_train, cmap="jet", edgecolor="k", label="Train"
)
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap="jet", marker="x", label="Test")

# Etiquetas del gr√°fico
plt.xlabel("S√©palo largo (normalizado)")
plt.ylabel("S√©palo ancho (normalizado)")
plt.title("üåº Frontera de decisi√≥n de Red Neuronal MLP - Dataset Iris")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
